{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:04:05,785\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-29 18:05:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:11.22        </td></tr>\n",
       "<tr><td>Memory:      </td><td>24.5/32.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/10 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  pruning_percentage</th><th style=\"text-align: right;\">  retraining_epochs</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_52b4a_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.000488752</td><td style=\"text-align: right;\">            0.242432</td><td style=\"text-align: right;\">                  7</td></tr>\n",
       "<tr><td>train_model_52b4a_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.000496709</td><td style=\"text-align: right;\">            0.705944</td><td style=\"text-align: right;\">                  3</td></tr>\n",
       "<tr><td>train_model_52b4a_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.000286756</td><td style=\"text-align: right;\">            0.400921</td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>train_model_52b4a_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.0010558  </td><td style=\"text-align: right;\">            0.659698</td><td style=\"text-align: right;\">                  8</td></tr>\n",
       "<tr><td>train_model_52b4a_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00190987 </td><td style=\"text-align: right;\">            0.332618</td><td style=\"text-align: right;\">                  2</td></tr>\n",
       "<tr><td>train_model_52b4a_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00022964 </td><td style=\"text-align: right;\">            0.767099</td><td style=\"text-align: right;\">                  7</td></tr>\n",
       "<tr><td>train_model_52b4a_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00242692 </td><td style=\"text-align: right;\">            0.615977</td><td style=\"text-align: right;\">                  2</td></tr>\n",
       "<tr><td>train_model_52b4a_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.000883163</td><td style=\"text-align: right;\">            0.588201</td><td style=\"text-align: right;\">                  2</td></tr>\n",
       "<tr><td>train_model_52b4a_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00822041 </td><td style=\"text-align: right;\">            0.639556</td><td style=\"text-align: right;\">                  4</td></tr>\n",
       "<tr><td>train_model_52b4a_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00011893 </td><td style=\"text-align: right;\">            0.25938 </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>train_model_52b4a_00010</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.000193073</td><td style=\"text-align: right;\">            0.589644</td><td style=\"text-align: right;\">                  1</td></tr>\n",
       "<tr><td>train_model_52b4a_00011</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00458301 </td><td style=\"text-align: right;\">            0.518638</td><td style=\"text-align: right;\">                  8</td></tr>\n",
       "<tr><td>train_model_52b4a_00012</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00134475 </td><td style=\"text-align: right;\">            0.451966</td><td style=\"text-align: right;\">                  7</td></tr>\n",
       "<tr><td>train_model_52b4a_00013</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.000761742</td><td style=\"text-align: right;\">            0.61661 </td><td style=\"text-align: right;\">                  3</td></tr>\n",
       "<tr><td>train_model_52b4a_00014</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00634518 </td><td style=\"text-align: right;\">            0.298666</td><td style=\"text-align: right;\">                  1</td></tr>\n",
       "<tr><td>train_model_52b4a_00015</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00634232 </td><td style=\"text-align: right;\">            0.134118</td><td style=\"text-align: right;\">                  6</td></tr>\n",
       "<tr><td>train_model_52b4a_00016</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00203547 </td><td style=\"text-align: right;\">            0.223935</td><td style=\"text-align: right;\">                  8</td></tr>\n",
       "<tr><td>train_model_52b4a_00017</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.000365633</td><td style=\"text-align: right;\">            0.212355</td><td style=\"text-align: right;\">                  7</td></tr>\n",
       "<tr><td>train_model_52b4a_00018</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00152798 </td><td style=\"text-align: right;\">            0.676553</td><td style=\"text-align: right;\">                  6</td></tr>\n",
       "<tr><td>train_model_52b4a_00019</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">    0.00718864 </td><td style=\"text-align: right;\">            0.581419</td><td style=\"text-align: right;\">                  2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:05:17,072\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-11-29 18:05:17,078\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05' in 0.0051s.\n",
      "2024-11-29 18:05:17,100\tINFO tune.py:1041 -- Total run time: 71.31 seconds (71.22 seconds for the tuning loop).\n",
      "2024-11-29 18:05:17,100\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-11-29 18:05:17,106\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 20 trial(s):\n",
      "- train_model_52b4a_00000: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00000: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00000_0_learning_rate=0.0005,pruning_percentage=0.2424,retraining_epochs=7_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00001: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00001: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00001_1_learning_rate=0.0005,pruning_percentage=0.7059,retraining_epochs=3_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00002: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00002: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00002_2_learning_rate=0.0003,pruning_percentage=0.4009,retraining_epochs=5_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00003: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00003: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00003_3_learning_rate=0.0011,pruning_percentage=0.6597,retraining_epochs=8_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00004: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00004: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00004_4_learning_rate=0.0019,pruning_percentage=0.3326,retraining_epochs=2_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00005: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00005: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00005_5_learning_rate=0.0002,pruning_percentage=0.7671,retraining_epochs=7_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00006: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00006: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00006_6_learning_rate=0.0024,pruning_percentage=0.6160,retraining_epochs=2_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00007: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00007: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00007_7_learning_rate=0.0009,pruning_percentage=0.5882,retraining_epochs=2_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00008: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00008: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00008_8_learning_rate=0.0082,pruning_percentage=0.6396,retraining_epochs=4_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00009: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00009: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00009_9_learning_rate=0.0001,pruning_percentage=0.2594,retraining_epochs=5_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00010: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00010: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00010_10_learning_rate=0.0002,pruning_percentage=0.5896,retraining_epochs=1_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00011: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00011: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00011_11_learning_rate=0.0046,pruning_percentage=0.5186,retraining_epochs=8_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00012: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00012: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00012_12_learning_rate=0.0013,pruning_percentage=0.4520,retraining_epochs=7_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00013: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00013: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00013_13_learning_rate=0.0008,pruning_percentage=0.6166,retraining_epochs=3_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00014: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00014: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00014_14_learning_rate=0.0063,pruning_percentage=0.2987,retraining_epochs=1_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00015: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00015: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00015_15_learning_rate=0.0063,pruning_percentage=0.1341,retraining_epochs=6_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00016: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00016: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00016_16_learning_rate=0.0020,pruning_percentage=0.2239,retraining_epochs=8_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00017: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00017: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00017_17_learning_rate=0.0004,pruning_percentage=0.2124,retraining_epochs=7_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00018: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00018: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00018_18_learning_rate=0.0015,pruning_percentage=0.6766,retraining_epochs=6_2024-11-29_18-04-05')\n",
      "- train_model_52b4a_00019: FileNotFoundError('Could not fetch metrics for train_model_52b4a_00019: both result.json and progress.csv were not found at /Users/sidharrthnagappan/ray_results/train_model_2024-11-29_18-04-05/train_model_52b4a_00019_19_learning_rate=0.0072,pruning_percentage=0.5814,retraining_epochs=2_2024-11-29_18-04-05')\n",
      "2024-11-29 18:05:17,107\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ray import tune\n",
    "# from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Define a simple model for demonstration (e.g., LeNet for MNIST)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training function with model pruning\n",
    "def train_model(config):\n",
    "    pruning_percentage = config[\"pruning_percentage\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    retraining_epochs = config[\"retraining_epochs\"]\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            torch.nn.utils.prune.l1_unstructured(module, name='weight', amount=pruning_percentage)\n",
    "\n",
    "    for epoch in range(retraining_epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    accuracy = evaluate_model(model)\n",
    "    compression_ratio = calculate_compression_ratio(model)\n",
    "\n",
    "    tune.report(accuracy=accuracy, compression_ratio=compression_ratio, reward=accuracy - compression_ratio * 0.1)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def calculate_compression_ratio(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    pruned_params = sum(p.numel() for p in model.parameters() if hasattr(p, \"weight_mask\"))\n",
    "    return (total_params - pruned_params) / total_params\n",
    "\n",
    "# RLlib-based optimization\n",
    "def run_rllib_search():\n",
    "    # Define the search space\n",
    "    search_space = {\n",
    "        \"pruning_percentage\": tune.uniform(0.1, 0.8),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"retraining_epochs\": tune.randint(1, 10),\n",
    "    }\n",
    "\n",
    "    analysis = tune.run(\n",
    "        train_model,\n",
    "        config=search_space,\n",
    "        metric=\"reward\",\n",
    "        mode=\"max\",\n",
    "        num_samples=20,\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_rllib_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 17:59:00,073\tWARNING algorithm_config.py:4484 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "2024-11-29 17:59:00,074\tWARNING ppo.py:305 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/Users/sidharrthnagappan/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:567: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/sidharrthnagappan/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/sidharrthnagappan/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/sidharrthnagappan/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/sidharrthnagappan/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "/Users/sidharrthnagappan/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/Users/sidharrthnagappan/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "2024-11-29 17:59:22,204\tWARNING algorithm_config.py:4484 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "2024-11-29 17:59:22,204\tWARNING ppo.py:305 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2024-11-29 17:59:22,207\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "2024-11-29 17:59:22,217\tINFO trainable.py:161 -- Trainable.setup took 22.140 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-11-29 17:59:22,218\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-11-29 18:00:24,055\tERROR actor_manager.py:804 -- Ray error (The actor 83c6794e5a879b19dd98ca4d01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "2024-11-29 18:00:24,055\tERROR actor_manager.py:804 -- Ray error (The actor 43443ceacd277efbf370d9bd01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "2024-11-29 18:00:24,056\tERROR actor_manager.py:804 -- Ray error (The actor 0900c848171ed111d550a2a201000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "2024-11-29 18:00:24,057\tERROR actor_manager.py:804 -- Ray error (The actor 7f2d14bc95bbdc57d22b577701000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 4 out of service.\n",
      "2024-11-29 18:00:24,058\tERROR actor_manager.py:804 -- Ray error (The actor 1a688ba3bb2a4195840152a601000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 5 out of service.\n",
      "2024-11-29 18:00:24,058\tERROR actor_manager.py:635 -- The actor 83c6794e5a879b19dd98ca4d01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:00:24,059\tERROR actor_manager.py:635 -- The actor 43443ceacd277efbf370d9bd01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:00:24,059\tERROR actor_manager.py:635 -- The actor 0900c848171ed111d550a2a201000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:00:24,060\tERROR actor_manager.py:635 -- The actor 7f2d14bc95bbdc57d22b577701000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:00:24,060\tERROR actor_manager.py:635 -- The actor 1a688ba3bb2a4195840152a601000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:00:24,060\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: reward_mean={'timers': {'training_iteration_time_sec': 61.84202218055725, 'restore_workers_time_sec': 6.198883056640625e-06, 'training_step_time_sec': 61.84197497367859}, 'fault_tolerance': {'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2024-11-29_18-00-24', 'timestamp': 1732903224, 'time_this_iter_s': 61.84218978881836, 'time_total_s': 61.84218978881836, 'pid': 33553, 'hostname': 'user-116-99.svr-vpn-0.vpn.cl.cam.ac.uk', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, 'env': 'CompressionEnv', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 5, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'max_requests_in_flight_per_env_runner': 2, 'sample_timeout_s': None, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [128, 128], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'explore': True, 'exploration_config': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x15daa0040>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 61.84218978881836, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(30.97613636363636), 'ram_util_percent': np.float64(75.92159090909091)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:00:26,061\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: reward_mean={'timers': {'training_iteration_time_sec': 31.919841051101685, 'restore_workers_time_sec': 0.9971414804458618, 'training_step_time_sec': 30.92265748977661}, 'fault_tolerance': {'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'done': False, 'training_iteration': 2, 'trial_id': 'default', 'date': '2024-11-29_18-00-26', 'timestamp': 1732903226, 'time_this_iter_s': 1.997734785079956, 'time_total_s': 63.839924573898315, 'pid': 33553, 'hostname': 'user-116-99.svr-vpn-0.vpn.cl.cam.ac.uk', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, 'env': 'CompressionEnv', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 5, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'max_requests_in_flight_per_env_runner': 2, 'sample_timeout_s': None, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [128, 128], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'explore': True, 'exploration_config': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x15daa0040>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 63.839924573898315, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': np.float64(52.86666666666667), 'ram_util_percent': np.float64(74.73333333333333)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:00:28,059\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2024-11-29 18:00:28,074\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "2024-11-29 18:00:28,075\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "2024-11-29 18:00:28,075\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "2024-11-29 18:00:28,076\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 4 back into service.\n",
      "2024-11-29 18:00:28,078\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 5 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: reward_mean={'timers': {'training_iteration_time_sec': 21.942126353581745, 'restore_workers_time_sec': 1.3264740308125813, 'training_step_time_sec': 20.615607023239136}, 'fault_tolerance': {'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'done': False, 'training_iteration': 3, 'trial_id': 'default', 'date': '2024-11-29_18-00-28', 'timestamp': 1732903228, 'time_this_iter_s': 1.9871129989624023, 'time_total_s': 65.82703757286072, 'pid': 33553, 'hostname': 'user-116-99.svr-vpn-0.vpn.cl.cam.ac.uk', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, 'env': 'CompressionEnv', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 5, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'max_requests_in_flight_per_env_runner': 2, 'sample_timeout_s': None, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [128, 128], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'explore': True, 'exploration_config': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x15daa0040>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 65.82703757286072, 'iterations_since_restore': 3, 'perf': {'cpu_util_percent': np.float64(37.56666666666666), 'ram_util_percent': np.float64(76.43333333333334)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:01:30,111\tERROR actor_manager.py:804 -- Ray error (The actor 83c6794e5a879b19dd98ca4d01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "2024-11-29 18:01:30,112\tERROR actor_manager.py:804 -- Ray error (The actor 43443ceacd277efbf370d9bd01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "2024-11-29 18:01:30,112\tERROR actor_manager.py:804 -- Ray error (The actor 0900c848171ed111d550a2a201000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "2024-11-29 18:01:30,113\tERROR actor_manager.py:804 -- Ray error (The actor 7f2d14bc95bbdc57d22b577701000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 4 out of service.\n",
      "2024-11-29 18:01:30,114\tERROR actor_manager.py:804 -- Ray error (The actor 1a688ba3bb2a4195840152a601000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.), taking actor 5 out of service.\n",
      "2024-11-29 18:01:30,114\tERROR actor_manager.py:635 -- The actor 83c6794e5a879b19dd98ca4d01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:01:30,115\tERROR actor_manager.py:635 -- The actor 43443ceacd277efbf370d9bd01000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:01:30,115\tERROR actor_manager.py:635 -- The actor 0900c848171ed111d550a2a201000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:01:30,116\tERROR actor_manager.py:635 -- The actor 7f2d14bc95bbdc57d22b577701000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:01:30,116\tERROR actor_manager.py:635 -- The actor 1a688ba3bb2a4195840152a601000000 is unavailable: The actor is temporarily unavailable: UnexpectedSystemExit: Worker exits with an exit code 1.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2024-11-29 18:01:30,116\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: reward_mean={'timers': {'training_iteration_time_sec': 31.96889078617096, 'restore_workers_time_sec': 1.0025712251663208, 'training_step_time_sec': 30.966251254081726}, 'fault_tolerance': {'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 5}, 'done': False, 'training_iteration': 4, 'trial_id': 'default', 'date': '2024-11-29_18-01-30', 'timestamp': 1732903290, 'time_this_iter_s': 62.049379110336304, 'time_total_s': 127.87641668319702, 'pid': 33553, 'hostname': 'user-116-99.svr-vpn-0.vpn.cl.cam.ac.uk', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, 'env': 'CompressionEnv', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 5, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'max_requests_in_flight_per_env_runner': 2, 'sample_timeout_s': None, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [128, 128], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'explore': True, 'exploration_config': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x15daa0040>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 127.87641668319702, 'iterations_since_restore': 4, 'perf': {'cpu_util_percent': np.float64(28.868181818181814), 'ram_util_percent': np.float64(76.4034090909091)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 18:01:32,112\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2024-11-29 18:01:32,118\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "2024-11-29 18:01:32,119\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "2024-11-29 18:01:32,120\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "2024-11-29 18:01:32,120\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 4 back into service.\n",
      "2024-11-29 18:01:32,120\tWARNING actor_manager.py:817 -- Bringing previously unhealthy, now-healthy actor 5 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: reward_mean={'timers': {'training_iteration_time_sec': 25.97334804534912, 'restore_workers_time_sec': 1.2001412391662598, 'training_step_time_sec': 24.773144006729126}, 'fault_tolerance': {'num_healthy_workers': 0, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 5}, 'done': False, 'training_iteration': 5, 'trial_id': 'default', 'date': '2024-11-29_18-01-32', 'timestamp': 1732903292, 'time_this_iter_s': 1.9912559986114502, 'time_total_s': 129.86767268180847, 'pid': 33553, 'hostname': 'user-116-99.svr-vpn-0.vpn.cl.cam.ac.uk', 'node_ip': '127.0.0.1', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, 'env': 'CompressionEnv', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 5, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'max_requests_in_flight_per_env_runner': 2, 'sample_timeout_s': None, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [128, 128], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'explore': True, 'exploration_config': {}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x15daa0040>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 129.86767268180847, 'iterations_since_restore': 5, 'perf': {'cpu_util_percent': np.float64(42.800000000000004), 'ram_util_percent': np.float64(74.56666666666666)}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 158\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Train the algorithm\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# Run 50 iterations\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: reward_mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Save the trained policy\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:933\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    923\u001b[0m     (\n\u001b[1;32m    924\u001b[0m         train_results,\n\u001b[1;32m    925\u001b[0m         eval_results,\n\u001b[1;32m    926\u001b[0m         train_iter_ctx,\n\u001b[1;32m    927\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 933\u001b[0m     train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3498\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3494\u001b[0m \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[1;32m   3496\u001b[0m     \u001b[38;5;66;03m# TODO (sven): Should we reduce the different\u001b[39;00m\n\u001b[1;32m   3497\u001b[0m     \u001b[38;5;66;03m#  `training_step_results` over time with MetricsLogger.\u001b[39;00m\n\u001b[0;32m-> 3498\u001b[0m     training_step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_results:\n\u001b[1;32m   3501\u001b[0m     results \u001b[38;5;241m=\u001b[39m training_step_results\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:413\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@override\u001b[39m(Algorithm)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# New API stack (RLModule, Learner, EnvRunner, ConnectorV2).\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[0;32m--> 413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_new_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Old API stack (Policy, RolloutWorker, Connector, maybe RLModule,\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# maybe Learner).\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_step_old_api_stack()\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:434\u001b[0m, in \u001b[0;36mPPO._training_step_new_api_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     episodes, env_runner_results \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    425\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group,\n\u001b[1;32m    426\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m         _return_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m     episodes, env_runner_results \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runner_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_uses_new_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_return_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Return early if all our workers failed.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episodes:\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py:106\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, random_actions, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m    103\u001b[0m         stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_env_runner\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_return_metrics\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Nothing was returned (maybe all workers are stalling) or no healthy\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# remote workers left: Break.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# There is no point staying in this loop, since we will not be able to\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# get any new samples if we don't have any healthy remote workers left.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampled_data \u001b[38;5;129;01mor\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_healthy_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/env/env_runner_group.py:896\u001b[0m, in \u001b[0;36mEnvRunnerGroup.foreach_worker\u001b[0;34m(self, func, local_env_runner, healthy_only, remote_worker_ids, timeout_seconds, return_obj_refs, mark_healthy, local_worker)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids():\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m local_result\n\u001b[0;32m--> 896\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_actor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhealthy_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealthy_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m FaultTolerantActorManager\u001b[38;5;241m.\u001b[39mhandle_remote_call_result_errors(\n\u001b[1;32m    906\u001b[0m     remote_results, ignore_ray_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_ray_errors_on_env_runners\n\u001b[1;32m    907\u001b[0m )\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# With application errors handled, return good results.\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:452\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.foreach_actor\u001b[0;34m(self, func, healthy_only, remote_actor_ids, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    446\u001b[0m remote_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_actors(\n\u001b[1;32m    447\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    448\u001b[0m     remote_actor_ids\u001b[38;5;241m=\u001b[39mremote_actor_ids,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Collect remote request results (if available given timeout and/or errors).\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m _, remote_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_actor_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_actor_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmark_healthy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmark_healthy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remote_results\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py:770\u001b[0m, in \u001b[0;36mFaultTolerantActorManager._fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_calls:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], RemoteCallResults()\n\u001b[0;32m--> 770\u001b[0m readies, _ \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremote_calls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make sure remote results are fetched locally in parallel.\u001b[39;49;00m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_obj_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# Remote data should already be fetched to local object store at this point.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m RemoteCallResults()\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/lsdp_miniproject/lib/python3.11/site-packages/ray/_private/worker.py:2984\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ray_waitables, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2982\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2983\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2984\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_waitables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3816\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/includes/common.pxi:79\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "# Define a simple CNN model for compression\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the custom RL environment\n",
    "class CompressionEnv(gym.Env):  # Inherit from gymnasium.Env\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(CompressionEnv, self).__init__()\n",
    "\n",
    "        # Define action space: [pruning percentage, learning rate]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.1, 1e-4]), high=np.array([0.8, 1e-2]), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Define observation space: can include state info (e.g., current accuracy, size)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        # Load dataset\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "        )\n",
    "        self.train_data = datasets.MNIST(\n",
    "            root=\"data\", train=True, download=True, transform=transform\n",
    "        )\n",
    "        self.test_data = datasets.MNIST(\n",
    "            root=\"data\", train=False, download=True, transform=transform\n",
    "        )\n",
    "        self.train_loader = DataLoader(self.train_data, batch_size=64, shuffle=True)\n",
    "        self.test_loader = DataLoader(self.test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Initialize model and other variables\n",
    "        self.model = SimpleCNN()\n",
    "        self.initial_params = sum(p.numel() for p in self.model.parameters())\n",
    "        self.state = np.array(\n",
    "            [0, 0, 0]\n",
    "        )  # Placeholder state: accuracy, compression ratio, steps\n",
    "        self.episode_length = 10\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset the environment for a new episode\n",
    "        super().reset(seed=seed)\n",
    "        self.model = SimpleCNN()  # Reinitialize the model\n",
    "        self.state = np.array([0, 0, 0])  # Reset state\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        pruning_percentage, learning_rate = action\n",
    "\n",
    "        # Apply pruning\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                torch.nn.utils.prune.l1_unstructured(\n",
    "                    module, name=\"weight\", amount=pruning_percentage\n",
    "                )\n",
    "\n",
    "        # Retrain the model\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        self.model.train()\n",
    "        for epoch in range(1):  # Train for 1 epoch per step\n",
    "            for data, target in self.train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = self.evaluate_model()\n",
    "        compression_ratio = self.calculate_compression_ratio()\n",
    "\n",
    "        # Update state\n",
    "        self.state = np.array([accuracy, compression_ratio, self.state[2] + 1])\n",
    "\n",
    "        # Reward: prioritize accuracy but incentivize compression\n",
    "        reward = accuracy - 0.1 * compression_ratio\n",
    "\n",
    "        done = self.state[2] >= self.episode_length\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                output = self.model(data)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        print(f\"Accuracy: {correct / total}\")\n",
    "        return correct / total\n",
    "\n",
    "    def calculate_compression_ratio(self):\n",
    "        pruned_params = sum(\n",
    "            p.numel() for p in self.model.parameters() if hasattr(p, \"weight_mask\")\n",
    "        )\n",
    "        return (self.initial_params - pruned_params) / self.initial_params\n",
    "\n",
    "\n",
    "# Register the environment with RLlib\n",
    "def create_env(config):\n",
    "    return CompressionEnv(config)\n",
    "\n",
    "\n",
    "register_env(\"CompressionEnv\", create_env)\n",
    "\n",
    "# RLlib Training Configuration\n",
    "if __name__ == \"__main__\":\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(env=\"CompressionEnv\")\n",
    "        .framework(\"torch\")\n",
    "        .api_stack(\n",
    "            enable_rl_module_and_learner=True, enable_env_runner_and_connector_v2=True\n",
    "        )\n",
    "        .env_runners(\n",
    "            num_env_runners=5, sample_timeout_s=None\n",
    "        )  # Set the number of environment runners\n",
    "        .training(model={\"fcnet_hiddens\": [128, 128]})\n",
    "    )\n",
    "\n",
    "    # Build the PPO algorithm\n",
    "    algo = config.build()\n",
    "\n",
    "    # Train the algorithm\n",
    "    for i in range(50):  # Run 50 iterations\n",
    "        result = algo.train()\n",
    "        print(f\"Iteration {i}: reward_mean={result}\")\n",
    "\n",
    "    # Save the trained policy\n",
    "    algo.save(\"./ppo_compression_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# read npz file in /Users/sidharrthnagappan/Downloads\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(\"/Users/sidharrthnagappan/Downloads/305_P.npz\")\n",
    "print(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read csv, divided by ;\n",
    "data = pd.read_csv(\"/Users/sidharrthnagappan/Downloads/452_OpenSMILE2.3.0_mfcc.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>frameTime</th>\n",
       "      <th>pcm_fftMag_mfcc[0]</th>\n",
       "      <th>pcm_fftMag_mfcc[1]</th>\n",
       "      <th>pcm_fftMag_mfcc[2]</th>\n",
       "      <th>pcm_fftMag_mfcc[3]</th>\n",
       "      <th>pcm_fftMag_mfcc[4]</th>\n",
       "      <th>pcm_fftMag_mfcc[5]</th>\n",
       "      <th>pcm_fftMag_mfcc[6]</th>\n",
       "      <th>pcm_fftMag_mfcc[7]</th>\n",
       "      <th>...</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[3]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[4]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[5]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[6]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[7]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[8]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[9]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[10]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[11]</th>\n",
       "      <th>pcm_fftMag_mfcc_de_de[12]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-9.915971</td>\n",
       "      <td>-8.620132</td>\n",
       "      <td>2.039084</td>\n",
       "      <td>-5.163440</td>\n",
       "      <td>2.006905</td>\n",
       "      <td>18.294490</td>\n",
       "      <td>22.194500</td>\n",
       "      <td>15.895390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152880</td>\n",
       "      <td>0.489295</td>\n",
       "      <td>0.710587</td>\n",
       "      <td>0.540104</td>\n",
       "      <td>0.130175</td>\n",
       "      <td>0.031273</td>\n",
       "      <td>-0.916245</td>\n",
       "      <td>-1.907472</td>\n",
       "      <td>-0.958420</td>\n",
       "      <td>-0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-6.664284</td>\n",
       "      <td>-4.194248</td>\n",
       "      <td>-4.648168</td>\n",
       "      <td>-3.684016</td>\n",
       "      <td>-4.954080</td>\n",
       "      <td>-6.889012</td>\n",
       "      <td>7.564898</td>\n",
       "      <td>-0.844071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203163</td>\n",
       "      <td>0.871625</td>\n",
       "      <td>2.274281</td>\n",
       "      <td>1.531581</td>\n",
       "      <td>1.520242</td>\n",
       "      <td>0.048998</td>\n",
       "      <td>-1.585407</td>\n",
       "      <td>-2.866117</td>\n",
       "      <td>-1.770200</td>\n",
       "      <td>0.156029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-6.332829</td>\n",
       "      <td>0.685269</td>\n",
       "      <td>-0.365634</td>\n",
       "      <td>2.003712</td>\n",
       "      <td>0.390207</td>\n",
       "      <td>-0.603974</td>\n",
       "      <td>-1.424609</td>\n",
       "      <td>2.812038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900279</td>\n",
       "      <td>0.538887</td>\n",
       "      <td>2.665507</td>\n",
       "      <td>2.607594</td>\n",
       "      <td>2.207535</td>\n",
       "      <td>-0.088446</td>\n",
       "      <td>-1.505144</td>\n",
       "      <td>-1.711007</td>\n",
       "      <td>-1.432200</td>\n",
       "      <td>0.416258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-9.073243</td>\n",
       "      <td>0.205635</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>1.457144</td>\n",
       "      <td>0.118186</td>\n",
       "      <td>-5.632328</td>\n",
       "      <td>2.595091</td>\n",
       "      <td>-9.695595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.727857</td>\n",
       "      <td>0.464294</td>\n",
       "      <td>2.244686</td>\n",
       "      <td>2.481748</td>\n",
       "      <td>2.309757</td>\n",
       "      <td>-0.095080</td>\n",
       "      <td>-0.493684</td>\n",
       "      <td>0.328701</td>\n",
       "      <td>-0.890489</td>\n",
       "      <td>0.639589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-5.547980</td>\n",
       "      <td>3.337821</td>\n",
       "      <td>-0.236531</td>\n",
       "      <td>2.846203</td>\n",
       "      <td>5.405900</td>\n",
       "      <td>4.883540</td>\n",
       "      <td>7.485616</td>\n",
       "      <td>7.462225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057012</td>\n",
       "      <td>0.143035</td>\n",
       "      <td>0.048417</td>\n",
       "      <td>1.076983</td>\n",
       "      <td>0.554909</td>\n",
       "      <td>0.119702</td>\n",
       "      <td>2.025989</td>\n",
       "      <td>2.261990</td>\n",
       "      <td>-0.292087</td>\n",
       "      <td>0.302947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88903</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>889.03</td>\n",
       "      <td>6.596205</td>\n",
       "      <td>9.779520</td>\n",
       "      <td>17.572750</td>\n",
       "      <td>19.726100</td>\n",
       "      <td>15.150250</td>\n",
       "      <td>12.101650</td>\n",
       "      <td>25.052470</td>\n",
       "      <td>20.088950</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.633085</td>\n",
       "      <td>-2.086792</td>\n",
       "      <td>-1.583521</td>\n",
       "      <td>-0.576014</td>\n",
       "      <td>0.486673</td>\n",
       "      <td>0.896709</td>\n",
       "      <td>1.213446</td>\n",
       "      <td>2.731232</td>\n",
       "      <td>2.818074</td>\n",
       "      <td>-0.922672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88904</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>889.04</td>\n",
       "      <td>-1.099818</td>\n",
       "      <td>5.529945</td>\n",
       "      <td>11.248440</td>\n",
       "      <td>15.844810</td>\n",
       "      <td>12.723560</td>\n",
       "      <td>4.295323</td>\n",
       "      <td>20.629760</td>\n",
       "      <td>10.558960</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.572607</td>\n",
       "      <td>-0.766718</td>\n",
       "      <td>-1.150338</td>\n",
       "      <td>-1.013006</td>\n",
       "      <td>-0.534065</td>\n",
       "      <td>-1.417233</td>\n",
       "      <td>-1.234026</td>\n",
       "      <td>0.508356</td>\n",
       "      <td>2.143103</td>\n",
       "      <td>0.315381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88905</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>889.05</td>\n",
       "      <td>-4.075032</td>\n",
       "      <td>2.090782</td>\n",
       "      <td>7.954472</td>\n",
       "      <td>12.467060</td>\n",
       "      <td>8.319486</td>\n",
       "      <td>3.370588</td>\n",
       "      <td>28.262950</td>\n",
       "      <td>10.999320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.499376</td>\n",
       "      <td>0.946104</td>\n",
       "      <td>-0.529290</td>\n",
       "      <td>-1.563444</td>\n",
       "      <td>-1.549245</td>\n",
       "      <td>-3.077513</td>\n",
       "      <td>-1.540916</td>\n",
       "      <td>-0.110525</td>\n",
       "      <td>1.013761</td>\n",
       "      <td>1.093440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88906</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>889.06</td>\n",
       "      <td>-4.853426</td>\n",
       "      <td>-5.538296</td>\n",
       "      <td>4.713272</td>\n",
       "      <td>5.192731</td>\n",
       "      <td>-3.556020</td>\n",
       "      <td>-0.729859</td>\n",
       "      <td>30.653840</td>\n",
       "      <td>22.930960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742765</td>\n",
       "      <td>1.914226</td>\n",
       "      <td>0.516001</td>\n",
       "      <td>-1.311308</td>\n",
       "      <td>-1.343467</td>\n",
       "      <td>-1.682199</td>\n",
       "      <td>-1.308228</td>\n",
       "      <td>-1.364408</td>\n",
       "      <td>-0.566124</td>\n",
       "      <td>0.827624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88907</th>\n",
       "      <td>'unknown'</td>\n",
       "      <td>889.07</td>\n",
       "      <td>-0.771481</td>\n",
       "      <td>-4.398062</td>\n",
       "      <td>1.470315</td>\n",
       "      <td>8.706748</td>\n",
       "      <td>6.052188</td>\n",
       "      <td>-6.606388</td>\n",
       "      <td>14.493950</td>\n",
       "      <td>14.225370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904460</td>\n",
       "      <td>1.037778</td>\n",
       "      <td>1.049099</td>\n",
       "      <td>0.159067</td>\n",
       "      <td>-0.292597</td>\n",
       "      <td>0.207480</td>\n",
       "      <td>-0.139268</td>\n",
       "      <td>-1.012245</td>\n",
       "      <td>-1.720451</td>\n",
       "      <td>0.130879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88908 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  frameTime  pcm_fftMag_mfcc[0]  pcm_fftMag_mfcc[1]  \\\n",
       "0      'unknown'       0.00           -9.915971           -8.620132   \n",
       "1      'unknown'       0.01           -6.664284           -4.194248   \n",
       "2      'unknown'       0.02           -6.332829            0.685269   \n",
       "3      'unknown'       0.03           -9.073243            0.205635   \n",
       "4      'unknown'       0.04           -5.547980            3.337821   \n",
       "...          ...        ...                 ...                 ...   \n",
       "88903  'unknown'     889.03            6.596205            9.779520   \n",
       "88904  'unknown'     889.04           -1.099818            5.529945   \n",
       "88905  'unknown'     889.05           -4.075032            2.090782   \n",
       "88906  'unknown'     889.06           -4.853426           -5.538296   \n",
       "88907  'unknown'     889.07           -0.771481           -4.398062   \n",
       "\n",
       "       pcm_fftMag_mfcc[2]  pcm_fftMag_mfcc[3]  pcm_fftMag_mfcc[4]  \\\n",
       "0                2.039084           -5.163440            2.006905   \n",
       "1               -4.648168           -3.684016           -4.954080   \n",
       "2               -0.365634            2.003712            0.390207   \n",
       "3                0.152932            1.457144            0.118186   \n",
       "4               -0.236531            2.846203            5.405900   \n",
       "...                   ...                 ...                 ...   \n",
       "88903           17.572750           19.726100           15.150250   \n",
       "88904           11.248440           15.844810           12.723560   \n",
       "88905            7.954472           12.467060            8.319486   \n",
       "88906            4.713272            5.192731           -3.556020   \n",
       "88907            1.470315            8.706748            6.052188   \n",
       "\n",
       "       pcm_fftMag_mfcc[5]  pcm_fftMag_mfcc[6]  pcm_fftMag_mfcc[7]  ...  \\\n",
       "0               18.294490           22.194500           15.895390  ...   \n",
       "1               -6.889012            7.564898           -0.844071  ...   \n",
       "2               -0.603974           -1.424609            2.812038  ...   \n",
       "3               -5.632328            2.595091           -9.695595  ...   \n",
       "4                4.883540            7.485616            7.462225  ...   \n",
       "...                   ...                 ...                 ...  ...   \n",
       "88903           12.101650           25.052470           20.088950  ...   \n",
       "88904            4.295323           20.629760           10.558960  ...   \n",
       "88905            3.370588           28.262950           10.999320  ...   \n",
       "88906           -0.729859           30.653840           22.930960  ...   \n",
       "88907           -6.606388           14.493950           14.225370  ...   \n",
       "\n",
       "       pcm_fftMag_mfcc_de_de[3]  pcm_fftMag_mfcc_de_de[4]  \\\n",
       "0                      0.152880                  0.489295   \n",
       "1                     -0.203163                  0.871625   \n",
       "2                     -0.900279                  0.538887   \n",
       "3                     -0.727857                  0.464294   \n",
       "4                     -0.057012                  0.143035   \n",
       "...                         ...                       ...   \n",
       "88903                 -1.633085                 -2.086792   \n",
       "88904                 -1.572607                 -0.766718   \n",
       "88905                 -0.499376                  0.946104   \n",
       "88906                  0.742765                  1.914226   \n",
       "88907                  0.904460                  1.037778   \n",
       "\n",
       "       pcm_fftMag_mfcc_de_de[5]  pcm_fftMag_mfcc_de_de[6]  \\\n",
       "0                      0.710587                  0.540104   \n",
       "1                      2.274281                  1.531581   \n",
       "2                      2.665507                  2.607594   \n",
       "3                      2.244686                  2.481748   \n",
       "4                      0.048417                  1.076983   \n",
       "...                         ...                       ...   \n",
       "88903                 -1.583521                 -0.576014   \n",
       "88904                 -1.150338                 -1.013006   \n",
       "88905                 -0.529290                 -1.563444   \n",
       "88906                  0.516001                 -1.311308   \n",
       "88907                  1.049099                  0.159067   \n",
       "\n",
       "       pcm_fftMag_mfcc_de_de[7]  pcm_fftMag_mfcc_de_de[8]  \\\n",
       "0                      0.130175                  0.031273   \n",
       "1                      1.520242                  0.048998   \n",
       "2                      2.207535                 -0.088446   \n",
       "3                      2.309757                 -0.095080   \n",
       "4                      0.554909                  0.119702   \n",
       "...                         ...                       ...   \n",
       "88903                  0.486673                  0.896709   \n",
       "88904                 -0.534065                 -1.417233   \n",
       "88905                 -1.549245                 -3.077513   \n",
       "88906                 -1.343467                 -1.682199   \n",
       "88907                 -0.292597                  0.207480   \n",
       "\n",
       "       pcm_fftMag_mfcc_de_de[9]  pcm_fftMag_mfcc_de_de[10]  \\\n",
       "0                     -0.916245                  -1.907472   \n",
       "1                     -1.585407                  -2.866117   \n",
       "2                     -1.505144                  -1.711007   \n",
       "3                     -0.493684                   0.328701   \n",
       "4                      2.025989                   2.261990   \n",
       "...                         ...                        ...   \n",
       "88903                  1.213446                   2.731232   \n",
       "88904                 -1.234026                   0.508356   \n",
       "88905                 -1.540916                  -0.110525   \n",
       "88906                 -1.308228                  -1.364408   \n",
       "88907                 -0.139268                  -1.012245   \n",
       "\n",
       "       pcm_fftMag_mfcc_de_de[11]  pcm_fftMag_mfcc_de_de[12]  \n",
       "0                      -0.958420                  -0.135700  \n",
       "1                      -1.770200                   0.156029  \n",
       "2                      -1.432200                   0.416258  \n",
       "3                      -0.890489                   0.639589  \n",
       "4                      -0.292087                   0.302947  \n",
       "...                          ...                        ...  \n",
       "88903                   2.818074                  -0.922672  \n",
       "88904                   2.143103                   0.315381  \n",
       "88905                   1.013761                   1.093440  \n",
       "88906                  -0.566124                   0.827624  \n",
       "88907                  -1.720451                   0.130879  \n",
       "\n",
       "[88908 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdp_miniproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
